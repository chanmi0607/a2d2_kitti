import os
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
from tqdm import tqdm
from collections import Counter
import cv2
from multiprocessing import Pool, cpu_count

# --- í•„ìš”í•œ ëª¨ë“ˆë§Œ import ---
from pcdet.datasets.processor.ground_removal import remove_ground_open3d, read_kitti_bin
from pcdet.models.detectors.bev_utils import pointcloud_to_bev
from pcdet.models.dense_heads.clustering_utils import cluster_bev_image
from rf_gt_utils import get_a2d2_gt_boxes # KITTI GT ë¡œë” (ì´ì „ íŒŒì¼ ì´ë¦„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)

# --- í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
def calculate_iou(boxA, boxB):
    """ë‘ ë°”ìš´ë”© ë°•ìŠ¤(x,y,w,h)ì˜ IoUë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])
    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])
    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = boxA[2] * boxA[3]
    boxBArea = boxB[2] * boxB[3]
    iou = interArea / float(boxAArea + boxBArea - interArea) if (boxAArea + boxBArea - interArea) > 0 else 0
    return iou

def extract_features_from_clusters(clusters, resolution, non_ground_points, bev_x_range, bev_y_range):
    """í´ëŸ¬ìŠ¤í„°ì—ì„œ 3D í†µê³„ íŠ¹ì§•ì„ í¬í•¨í•œ ë²¡í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    all_features, valid_clusters = [], []
    for (x, y, w_pixels, h_pixels) in clusters:
        x_max_world = bev_x_range[1] - (y * resolution)
        x_min_world = bev_x_range[1] - ((y + h_pixels) * resolution)
        y_max_world = bev_y_range[1] - (x * resolution)
        y_min_world = bev_y_range[1] - ((x + w_pixels) * resolution)
        mask = np.where(
            (non_ground_points[:, 0] >= x_min_world) & (non_ground_points[:, 0] < x_max_world) &
            (non_ground_points[:, 1] >= y_min_world) & (non_ground_points[:, 1] < y_max_world)
        )
        points_in_box = non_ground_points[mask]
        if len(points_in_box) < 2: continue
        
        w, l = w_pixels * resolution, h_pixels * resolution
        h = np.max(points_in_box[:, 2]) - np.min(points_in_box[:, 2])
        z_std = np.std(points_in_box[:, 2])
        
        feature_vector = [w, l, len(points_in_box), h, z_std]
        all_features.append(feature_vector)
        valid_clusters.append((x, y, w_pixels, h_pixels))
    return valid_clusters, np.array(all_features) if all_features else np.array([])

# ========================== ì„¤ì • (ì „ì—­ ë³€ìˆ˜) ==========================
BASE_DIR = "/home/a/OpenPCDet/data/a2d2/training"
BIN_DATA_DIR = os.path.join(BASE_DIR, "velodyne")
LABEL_DIR = os.path.join(BASE_DIR, "label_2")
CALIB_DIR = os.path.join(BASE_DIR, "calib")

MODEL_SAVE_PATH = "advanced_model.joblib"
MAPPING_SAVE_PATH = "advanced_class_mapping.json"
IOU_THRESHOLD = 0.3
VISUALIZE_MATCHING = True
VISUALIZATION_OUTPUT_DIR = "training_visualizations_a2d2"

CLASS_MAPPING = {
    'Car': 0, 'Pedestrian': 1, 'Truck': 2, 'Cyclist': 3, 'Bicycle': 4,
    'Bus': 5, 'UtilityVehicle': 6, 'Trailer': 7, 'MotorBiker': 8, 'Background': 9
}
BEV_X_RANGE, BEV_Y_RANGE, BEV_RESOLUTION, MIN_CLUSTER_AREA = (0, 70.4), (-40, 40), 0.1, 15
# ======================================================================

def process_file(bin_name):
    """ë‹¨ì¼ .bin íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì‘ì—…ì í•¨ìˆ˜ (KITTI ê¸°ì¤€)"""
    try:
        # --- 0. íŒŒì¼ ê²½ë¡œ ì¤€ë¹„ ---
        file_name_base = os.path.splitext(bin_name)[0]
        bin_path = os.path.join(BIN_DATA_DIR, f"{file_name_base}.bin")
        label_path = os.path.join(LABEL_DIR, f"{file_name_base}.txt")
        calib_path = os.path.join(CALIB_DIR, f"{file_name_base}.txt")

        if not (os.path.exists(label_path) and os.path.exists(calib_path)): return None

        # --- 1. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: í´ëŸ¬ìŠ¤í„° í›„ë³´ ìƒì„± ---
        pcd = read_kitti_bin(bin_path)
        original_points = np.asarray(pcd.points)
        if original_points.shape[0] == 0: return None

        non_ground_points = remove_ground_open3d(original_points)
        if non_ground_points.shape[0] == 0: return None
        
        bev_image = pointcloud_to_bev(points=non_ground_points, x_range=BEV_X_RANGE, y_range=BEV_Y_RANGE, resolution=BEV_RESOLUTION)
        if bev_image is None: return None

        kernel = np.ones((3, 3), np.uint8)
        processed_bev_image = cv2.morphologyEx(bev_image, cv2.MORPH_CLOSE, kernel)
        
        clusters, _ = cluster_bev_image(processed_bev_image, min_area_threshold=MIN_CLUSTER_AREA)
        if not clusters: return None

        # --- 2. ì •ë‹µ(GT) ë°•ìŠ¤ ë¡œë“œ (rf_gt_utils ì‚¬ìš©) ---
        gt_boxes_with_corners = get_a2d2_gt_boxes(label_path, calib_path, BEV_X_RANGE, BEV_Y_RANGE, BEV_RESOLUTION)
        if not gt_boxes_with_corners: return None
        
        # --- 3. í´ëŸ¬ìŠ¤í„°ë¡œë¶€í„° íŠ¹ì§• ì¶”ì¶œ ---
        valid_clusters, features_from_clusters = extract_features_from_clusters(
            clusters, BEV_RESOLUTION, non_ground_points, BEV_X_RANGE, BEV_Y_RANGE
        )
        if features_from_clusters.shape[0] == 0: return None

        # --- 4. IoU ê¸°ë°˜ ìë™ ë¼ë²¨ë§ ---
        file_features, file_labels, clusters_for_vis = [], [], []
        for i, cluster_box in enumerate(valid_clusters):
            best_iou, best_gt_class_id = 0.0, -1
            for gt in gt_boxes_with_corners:
                gt_aligned_box = cv2.boundingRect(gt['corners'])
                iou = calculate_iou(cluster_box, gt_aligned_box)
                if iou > best_iou:
                    gt_class_str = gt['class']
                    if gt_class_str in ['Van']: gt_class_str = 'Car' # KITTI í´ë˜ìŠ¤ í†µí•©
                    
                    if gt_class_str in CLASS_MAPPING:
                        best_iou, best_gt_class_id = iou, CLASS_MAPPING[gt_class_str]

            label_to_assign = CLASS_MAPPING['Background']
            if best_iou >= IOU_THRESHOLD:
                label_to_assign = best_gt_class_id
            
            file_features.append(features_from_clusters[i])
            file_labels.append(label_to_assign)
            clusters_for_vis.append({'box': cluster_box, 'label': label_to_assign})

        # --- 5. ìµœì¢… ê²°ê³¼ ë°˜í™˜ ---
        return {
            "bin_name": bin_name, "bev_image": processed_bev_image,
            "gt_boxes": gt_boxes_with_corners, "clusters_for_vis": clusters_for_vis,
            "features": file_features, "labels": file_labels
        }
    except Exception:
        return None


def train_classifier():
    if VISUALIZE_MATCHING:
        os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)
    reverse_mapping = {v: k for k, v in CLASS_MAPPING.items()}

    all_features, all_labels = [], []
    bin_files = sorted([f for f in os.listdir(BIN_DATA_DIR) if f.endswith(".bin")])

    # --- ë³‘ë ¬ ì²˜ë¦¬ ---
    num_processes = cpu_count() - 1 if cpu_count() > 1 else 1
    print(f"{num_processes}ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    with Pool(processes=num_processes) as pool:
        results = list(tqdm(pool.imap_unordered(process_file, bin_files), total=len(bin_files), desc="í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘"))

    # --- ê²°ê³¼ ì·¨í•© & ì‹œê°í™” ---
    print("\nê²°ê³¼ ì·¨í•© ë° ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ì¤‘...")
    for result in tqdm(results, desc="ê²°ê³¼ ì²˜ë¦¬ ì¤‘"):
        if result is None: continue
        all_features.extend(result["features"])
        all_labels.extend(result["labels"])

        if VISUALIZE_MATCHING:
            vis_image = cv2.cvtColor(result["bev_image"], cv2.COLOR_GRAY2BGR)
            for gt in result["gt_boxes"]:
                # íšŒì „ëœ GT ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                cv2.drawContours(vis_image, [gt['corners'].astype(np.int32)], -1, (0, 255, 0), 1)
            for cluster in result["clusters_for_vis"]:
                x, y, w, h = cluster['box']
                label_int = cluster['label']
                class_name = reverse_mapping.get(label_int, "Unknown")
                color = (0, 0, 255) if class_name == 'Background' else (255, 0, 0)
                cv2.rectangle(vis_image, (x, y), (x + w, y + h), color, 2)
                cv2.putText(vis_image, class_name, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            save_path = os.path.join(VISUALIZATION_OUTPUT_DIR, f"{os.path.splitext(result['bin_name'])[0]}.png")
            cv2.imwrite(save_path, vis_image)

    # --- ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ---
    if not all_features:
        print("â—ï¸ Error: ìƒì„±ëœ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ ë° íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
        
    X = np.array(all_features)
    y = np.array(all_labels)
    
    print("\n" + "="*40)
    print("í´ë˜ìŠ¤ë³„ ë°ì´í„° ê°œìˆ˜:")
    for label_int, count in sorted(Counter(y).items()):
        print(f"- {reverse_mapping.get(label_int, 'Unknown')}: {count}ê°œ")
    print("="*40)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    print("\nRandomForest ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, verbose=10)
    model.fit(X_train, y_train)
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

    y_pred = model.predict(X_test)
    print("\n" + "="*40)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸:")
    print(classification_report(y_test, y_pred, labels=list(CLASS_MAPPING.values()), target_names=list(CLASS_MAPPING.keys()), zero_division=0))
    print("="*40)

    # --- ëª¨ë¸ ë° ë§¤í•‘ ì •ë³´ ì €ì¥ ---
    joblib.dump(model, MODEL_SAVE_PATH)
    with open(MAPPING_SAVE_PATH, 'w') as f:
        json.dump(CLASS_MAPPING, f, indent=4)
        
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_SAVE_PATH}")
    print(f"âœ… í´ë˜ìŠ¤ ë§µ ì €ì¥ ì™„ë£Œ: {MAPPING_SAVE_PATH}")


if __name__ == '__main__':
    # Pythonì˜ multiprocessingì„ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì¸ êµ¬ë¬¸
    train_classifier()